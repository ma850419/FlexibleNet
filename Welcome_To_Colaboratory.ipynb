{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma850419/FlexibleNet/blob/main/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from osgeo import gdal\n",
        "#filepath = r'/content/PRS_L1_STD_OFFL_20221217082013_20221217082018_0001_HCO_VNIR.tif'\n",
        "#filepath = r'/content/Sub_Sentinel2_.tif'\n",
        "filepath = r'/content/drive/MyDrive/Hyperion_0_2007-03-07.tif'\n",
        "# https://stackoverflow.com/questions/43684072/how-to-import-multiple-bands-from-an-image-into-numpy\n",
        "# Load one GeoTIFF image using GDAL\n",
        "dataset = gdal.Open(filepath)\n",
        "#nodata_value = dataset.GetRasterBand(1).GetNoDataValue()\n",
        "projInfo = dataset.GetProjection()\n",
        "trans = dataset.GetGeoTransform()\n",
        "print(projInfo,dataset.RasterXSize,dataset.RasterYSize,dataset.RasterCount )\n",
        "#print(me)\n",
        "#image = np.zeros(( dataset.RasterXSize,dataset.RasterYSize,dataset.RasterCount))\n",
        "image = dataset.ReadAsArray()\n",
        "image3 = np.transpose(image,(1,2,0))\n",
        "  # <class 'numpy.ndarray'>\n",
        "#print(image2.shape) \n",
        "#print(image.dtype)  \n",
        "#print(image.dtype)"
      ],
      "metadata": {
        "id": "L8pEg1GdIAjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image3.max())\n",
        "image3=image3/image3.max()\n",
        "print(image3.max())"
      ],
      "metadata": {
        "id": "mpDrpU23Dd-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install richdem\n",
        "!pip install pyhdf\n",
        "!pip install pysptools\n",
        "!pip install spectral\n",
        "!pip install surehyp\n"
      ],
      "metadata": {
        "id": "A3mfBmoA9J9a",
        "outputId": "30addccb-c274-4cf2-814b-651cb362db4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting richdem\n",
            "  Downloading richdem-0.3.4.tar.gz (329 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.4/329.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2,>=1.7 in /usr/local/lib/python3.9/dist-packages (from richdem) (1.22.4)\n",
            "Building wheels for collected packages: richdem\n",
            "  Building wheel for richdem (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for richdem: filename=richdem-0.3.4-cp39-cp39-linux_x86_64.whl size=7970685 sha256=dc057b986853baf4580627a9906869e61dc49804ab6beec4f3bd56d21de5dedf\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/e1/43/02e9a8e971c959066d5725ecb303dc327c6a9fc8a1b9d714c1\n",
            "Successfully built richdem\n",
            "Installing collected packages: richdem\n",
            "Successfully installed richdem-0.3.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyhdf\n",
            "  Downloading pyhdf-0.10.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.8/739.8 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pyhdf) (1.22.4)\n",
            "Installing collected packages: pyhdf\n",
            "Successfully installed pyhdf-0.10.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pysptools\n",
            "  Downloading pysptools-0.15.0.tar.gz (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pysptools\n",
            "  Building wheel for pysptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysptools: filename=pysptools-0.15.0-py3-none-any.whl size=8133744 sha256=f6d4a17c84a89fa69a9d1593487d2aa659db5576ac682ff5fc37fc5c008d0d11\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/18/a2/c6528dee884bf72153a558b496f84bc5ed1fb9759482945561\n",
            "Successfully built pysptools\n",
            "Installing collected packages: pysptools\n",
            "Successfully installed pysptools-0.15.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spectral in /usr/local/lib/python3.9/dist-packages (0.23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from spectral) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: surehyp in /usr/local/lib/python3.9/dist-packages (1.0.1.1)\n",
            "Requirement already satisfied: earthengine-api in /usr/local/lib/python3.9/dist-packages (from surehyp) (0.1.348)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from surehyp) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from surehyp) (4.65.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from surehyp) (1.10.1)\n",
            "Requirement already satisfied: spectral in /usr/local/lib/python3.9/dist-packages (from surehyp) (0.23.1)\n",
            "Requirement already satisfied: geetools in /usr/local/lib/python3.9/dist-packages (from surehyp) (0.6.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from surehyp) (1.5.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from surehyp) (4.7.0.72)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from surehyp) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from surehyp) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.9/dist-packages (from earthengine-api->surehyp) (2.8.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from earthengine-api->surehyp) (2.17.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from earthengine-api->surehyp) (2.27.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from earthengine-api->surehyp) (0.1.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from earthengine-api->surehyp) (0.21.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.12.1 in /usr/local/lib/python3.9/dist-packages (from earthengine-api->surehyp) (2.84.0)\n",
            "Requirement already satisfied: pyshp in /usr/local/lib/python3.9/dist-packages (from geetools->surehyp) (2.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->surehyp) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->surehyp) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->surehyp) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->surehyp) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->surehyp) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->surehyp) (5.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->surehyp) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->surehyp) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->surehyp) (23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->surehyp) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->surehyp) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->surehyp) (1.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.12.1->earthengine-api->surehyp) (2.11.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.12.1->earthengine-api->surehyp) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.4.1->earthengine-api->surehyp) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.4.1->earthengine-api->surehyp) (5.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.4.1->earthengine-api->surehyp) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.4.1->earthengine-api->surehyp) (0.2.8)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->surehyp) (3.15.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage->earthengine-api->surehyp) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage->earthengine-api->surehyp) (2.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->earthengine-api->surehyp) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->earthengine-api->surehyp) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->earthengine-api->surehyp) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->earthengine-api->surehyp) (2.0.12)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api->surehyp) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api->surehyp) (1.59.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.9/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->earthengine-api->surehyp) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->earthengine-api->surehyp) (0.4.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "rOPD4qBv4ooi",
        "outputId": "6a8d83bf-4221-4bc8-a618-f3c8c1e663f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from rasterio) (67.6.1)\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from rasterio) (22.2.0)\n",
            "Collecting affine\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.9/dist-packages (from rasterio) (1.22.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from rasterio) (2022.12.7)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.9/dist-packages (from rasterio) (8.1.3)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Collecting click-plugins\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.9/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n",
            "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.3.6 snuggs-1.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import rasterio\n",
        "import sys\n",
        "import ee\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "import sys, os\n",
        "import pandas as pd\n",
        "from scipy import interpolate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import surehyp.preprocess\n",
        "import surehyp.atmoCorrection\n",
        "import ee\n",
        "#ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "#ee.Initialize()\n",
        "\n",
        "def preprocess_radiance(fname,pathToL1Rmetadata,pathToL1Rimages,pathToL1Timages,pathToL1TimagesFiltered,pathOut,nameOut,destripingMethod='Pal',localDestriping=False,smileCorrectionOrder=2,checkSmile=False):\n",
        "    print('concatenate the L1T image')\n",
        "    surehyp.preprocess.processImage(fname,pathToL1Timages,pathToL1TimagesFiltered)\n",
        "\n",
        "    print('read the L1R image')\n",
        "    arrayL1R=surehyp.preprocess.readL1R(pathToL1Rimages+fname+'/',fname)\n",
        "\n",
        "    print('get the L1R image parameters')\n",
        "    metadata,bands,fwhms=surehyp.preprocess.getImageMetadata(pathToL1Rimages+fname+'/',fname)\n",
        "\n",
        "    print('separates VNIR and SWIR')\n",
        "    VNIR,VNIRb,VNIRfwhm,SWIR,SWIRb,SWIRfwhm=surehyp.preprocess.separating(arrayL1R,bands,fwhms)\n",
        "\n",
        "    print('converts DN to radiance')\n",
        "    VNIR,SWIR=surehyp.preprocess.DN2Radiance(VNIR,SWIR)\n",
        "\n",
        "    print('aligning VNIR and SWIR, part 1')\n",
        "    VNIR,SWIR=surehyp.preprocess.alignSWIR2VNIRpart1(VNIR,SWIR)\n",
        "\n",
        "    print('desmiling')\n",
        "    VNIR=surehyp.preprocess.smileCorrectionAll(VNIR,smileCorrectionOrder,check=checkSmile)\n",
        "    SWIR=surehyp.preprocess.smileCorrectionAll(SWIR,smileCorrectionOrder,check=checkSmile)\n",
        "\n",
        "    if destripingMethod=='Datt':\n",
        "        print('destriping - Datt (2003)')\n",
        "        VNIR=surehyp.preprocess.destriping(VNIR,'VNIR',0.11)\n",
        "        SWIR=surehyp.preprocess.destriping(SWIR,'SWIR',0.11)\n",
        "    elif destripingMethod=='Pal':\n",
        "        print('destriping -  Pal et al. (2020)')\n",
        "        VNIR,nc=surehyp.preprocess.destriping_quadratic(VNIR)\n",
        "        if localDestriping==True:\n",
        "            VNIR=surehyp.preprocess.destriping_local(VNIR,nc)\n",
        "        SWIR,nc=surehyp.preprocess.destriping_quadratic(SWIR)\n",
        "        if localDestriping==True:\n",
        "            SWIR=surehyp.preprocess.destriping_local(SWIR,nc)\n",
        "    else:\n",
        "        print('no destriping method selected -> no destriping')\n",
        "\n",
        "    #print('aligning VNIR and SWIR, part 2')\n",
        "    #VNIR,SWIR=surehyp.preprocess.alignSWIR2VNIRpart2(VNIR,VNIRb,SWIR,SWIRb)\n",
        "\n",
        "    print('assemble VNIR and SWIR')\n",
        "    arrayL1R,wavelengths,fwhms=surehyp.preprocess.concatenateImages(VNIR,VNIRb,VNIRfwhm,SWIR,SWIRb,SWIRfwhm)\n",
        "\n",
        "    print('smooth the cirrus bands for later thin cirrus removal') #may be necessary for Hyperion as an incomplete destriping for this band would then affect every other band during the thin cirrus removal\n",
        "    arrayL1R=surehyp.preprocess.smoothCirrusBand(arrayL1R,wavelengths)\n",
        "\n",
        "    print('georeference the corrected L1R data using the L1T data')\n",
        "    arrayL1Rgeoreferenced, metadataGeoreferenced=surehyp.preprocess.georeferencing(arrayL1R,pathToL1TimagesFiltered,fname)\n",
        "\n",
        "    print('save the processed image as an ENVI file')\n",
        "    surehyp.preprocess.savePreprocessedL1R(arrayL1Rgeoreferenced,wavelengths,fwhms,metadataGeoreferenced,pathToL1Rimages,pathToL1Rmetadata,metadata,fname,pathOut+nameOut)\n",
        "\n",
        "    for f in os.listdir(pathOut):\n",
        "        if (fname in f) and ('_tmp' in f):\n",
        "            os.remove(os.path.join(pathOut,f))\n",
        "\n",
        "    return pathOut+nameOut\n",
        "\n",
        "def atmosphericCorrection(pathToRadianceImage,pathToOutImage,stepAltit=1,stepTilt=15,stepWazim=30,demID='NRCan/CDEM',elevationName='elevation',topo=False,smartsAlbedoFilePath='./SMARTS2981-PC_Package/Albedo/Albedo.txt'):\n",
        "    print('open processed radiance image')\n",
        "    L,bands,fwhms,processing_metadata,metadata=surehyp.atmoCorrection.getImageAndParameters(pathToRadianceImage)\n",
        "\n",
        "    ####\n",
        "    #get info from the processing metadata for clearer visualization in the input of the subsequent functions\n",
        "    longit=processing_metadata['longit']\n",
        "    latit=processing_metadata['latit']\n",
        "    datestamp1=processing_metadata['datestamp1']\n",
        "    zenith=processing_metadata['zenith']\n",
        "    azimuth=processing_metadata['azimuth']\n",
        "    satelliteZenith=np.abs(processing_metadata['satelliteZenith'])\n",
        "    satelliteAzimuth=processing_metadata['satelliteAzimuth']\n",
        "\n",
        "    UL_lat=processing_metadata['UL_lat']\n",
        "    UL_lon=processing_metadata['UL_lon']\n",
        "    UR_lat=processing_metadata['UR_lat']\n",
        "    UR_lon=processing_metadata['UR_lon']\n",
        "    LL_lat=processing_metadata['LL_lat']\n",
        "    LL_lon=processing_metadata['LL_lon']\n",
        "    LR_lat=processing_metadata['LR_lat']\n",
        "    LR_lon=processing_metadata['LR_lon']\n",
        "\n",
        "    year=processing_metadata['year']\n",
        "    doy=processing_metadata['doy']\n",
        "    ####\n",
        "\n",
        "    if topo==True:\n",
        "        print('download DEM images from GEE')\n",
        "        path_to_dem = surehyp.atmoCorrection.getDEMimages(UL_lon,UL_lat,UR_lon,UR_lat,LR_lon,LR_lat,LL_lon,LL_lat,demID=demID,elevationName=elevationName)\n",
        "\n",
        "        print('reproject DEM images')\n",
        "        path_to_reprojected_dem = surehyp.atmoCorrection.reprojectDEM(pathToRadianceImage,path_elev=path_to_dem)\n",
        "\n",
        "        print('resampling')\n",
        "        path_elev=surehyp.atmoCorrection.matchResolution(pathToRadianceImage,path_elev=path_to_reprojected_dem)\n",
        "\n",
        "        print(\"extract the data corresponding to the Hyperion image's pixels\")\n",
        "        elev, slope, wazim=surehyp.atmoCorrection.extractDEMdata(pathToRadianceImage,path_elev=path_elev)\n",
        "    else:\n",
        "        slope=None\n",
        "        wazim=None\n",
        "\n",
        "    print('get clouds and shadows mask')\n",
        "    clearview, clouds, shadows = surehyp.atmoCorrection.cloudAndShadowsDetection(bands,L,latit,doy,satelliteZenith,zenith,azimuth,slope,wazim)\n",
        "\n",
        "    np.save(pathToOutImage+'_cloud_mask',clouds)\n",
        "    np.save(pathToOutImage+'_shadows_mask',shadows)\n",
        "    np.save(pathToOutImage+'_clearview_mask',clearview)\n",
        "\n",
        "    print('get haze spectrum')\n",
        "    L,Lhaze=surehyp.atmoCorrection.darkObjectDehazing(L,bands)\n",
        "\n",
        "    print('mask non clearview pixels')\n",
        "    L[clearview==0]=0\n",
        "\n",
        "    print('removal of thin cirrus')\n",
        "    L=surehyp.atmoCorrection.cirrusRemoval(bands,L,latit,doy,satelliteZenith,zenith,azimuth)\n",
        "\n",
        "\n",
        "\n",
        "    print('get average elevation of the scene from GEE')\n",
        "    altit=surehyp.atmoCorrection.getGEEdem(UL_lat,UL_lon,UR_lat,UR_lon,LL_lat,LL_lon,LR_lat,LR_lon,demID=demID,elevationName=elevationName)\n",
        "\n",
        "    print('get atmosphere content')\n",
        "    wv,o3,flag_no_o3=surehyp.atmoCorrection.getAtmosphericParameters(bands,L,datestamp1,year,doy,longit,latit,altit,satelliteZenith,zenith,azimuth)\n",
        "\n",
        "    if flag_no_o3==True:\n",
        "        IO3=1\n",
        "    else:\n",
        "        IO3=0\n",
        "\n",
        "    ########################################\n",
        "    # Atmospheric correction -- flat surface\n",
        "    print('obtain radiative transfer outputs')\n",
        "    #get the atmosphere parameters for the sun-ground section using the image acquisition time to determine sun angle\n",
        "    df=surehyp.atmoCorrection.runSMARTS(ALTIT=altit,LATIT=latit,IMASS=0,ZENITH=zenith,AZIM=azimuth,SUNCOR=surehyp.atmoCorrection.get_SUNCOR(doy),IH2O=0,WV=wv,IO3=IO3,IALT=0,AbO3=o3)\n",
        "    #get the atmosphere parameters for the ground-satellite section by setting the 'sun' (in SMARTS) at the satellite zenith position to get the transmittance over the correct optical path length\n",
        "    df_gs=surehyp.atmoCorrection.runSMARTS(ALTIT=altit,LATIT=0,LONGIT=0,IMASS=0,SUNCOR=surehyp.atmoCorrection.get_SUNCOR(doy),ITURB=5,ZENITH=satelliteZenith,AZIM=0,IH2O=0,WV=wv,IO3=IO3,IALT=0,AbO3=o3)\n",
        "\n",
        "    print('compute radiance to reflectance')\n",
        "    R=surehyp.atmoCorrection.computeLtoR(L,bands,df,df_gs)\n",
        "\n",
        "    if topo==False:\n",
        "        print('save the reflectance image')\n",
        "        surehyp.atmoCorrection.saveRimage(R,metadata,pathToOutImage)\n",
        "    else:\n",
        "        #######################################\n",
        "        #atmospheric correction -- rough terrain\n",
        "        print('write Albedo.txt file for SMARTS')\n",
        "        pathToAlbedoFile=surehyp.atmoCorrection.writeAlbedoFile(R,bands,pathOut=smartsAlbedoFilePath)\n",
        "\n",
        "        print('get scene background reflectance')\n",
        "        sp=pd.read_csv(pathToAlbedoFile,header=3,sep='\\s+')\n",
        "        w=sp.values[:,0]\n",
        "        r=sp.values[:,1]\n",
        "        f=interpolate.interp1d(w,r,bounds_error=False,fill_value='extrapolate')\n",
        "        rho_background=f(df['Wvlgth']*1E-3)\n",
        "\n",
        "        print('computing the LUT for the rough terrain correction')\n",
        "        R=surehyp.atmoCorrection.getDemReflectance(altitMap=elev,tiltMap=slope,wazimMap=wazim,stepAltit=stepAltit,stepTilt=stepTilt,stepWazim=stepWazim,latit=latit,IH2O=0,WV=wv,IO3=IO3,IALT=0,AbO3=o3,doy=doy,zenith=zenith,azimuth=azimuth,satelliteZenith=satelliteZenith,satelliteAzimuth=satelliteAzimuth,L=L,bands=bands,IALBDX=1,rho_background=rho_background)\n",
        "\n",
        "        print('MM topography correction')\n",
        "        R=surehyp.atmoCorrection.MM_topo_correction(R,bands,slope*np.pi/180,wazim*np.pi/180,zenith*np.pi/180,azimuth*np.pi/180)\n",
        "\n",
        "        print('save the reflectance image')\n",
        "        surehyp.atmoCorrection.saveRimage(R,metadata,pathToOutImage)\n",
        "\n",
        "    return pathToOutImage\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gC_u4YKauXPk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "#ee.Initialize()\n",
        "\n",
        "    # if using SMARTS 295, point to the *bat.exe\n",
        "    #os.environ['SMARTSPATH']='./SMARTS_295_PC/' #Path to the SMARTS folder\n",
        "    #surehyp.atmoCorrection.smartsVersion='smarts295'\n",
        "    #surehyp.atmoCorrection.smartsExecutable='smarts295bat.exe'\n",
        "    # else point to the regular exe\n",
        "    #os.environ['SMARTSPATH']='./SMARTS2981-PC_Package/' #Path to the SMARTS folder\n",
        "    #surehyp.atmoCorrection.smartsVersion='smarts298'\n",
        "    #surehyp.atmoCorrection.smartsExecutable='smarts2981_PC_64bit.exe'\n",
        "    pathToL1Rmetadata='./content/L1R/EO1H1740362009289110K4/EO1H1740362009289110K4.MET' #path to the Hyperion metadata file provided by the USGS\n",
        "    pathToL1Rimages=\"/content/L1R/\" #contains the uncompressed L1R folders ( that contain the .AUX,.jdr,.L1R,.MET, .fgdc files)\n",
        "    pathToL1Timages=\"/content/L1T/\" #contains the zip files of the L1T images (that contain a TIF file for each band)\n",
        "    pathToL1TimagesFiltered=\"/content/L1T/filteredImages/\" #contains the TIF L1R images, each TIF file containing all bands\n",
        "\n",
        "    pathOut='/content/OUT/' # folder where the outputs of SUREHYP will be written\n",
        "    fname='EO1H1740362009289110K4' # ID of the Hyperion image\n",
        "    nameOut=fname+'_test' # name of the corrected radiance image that will be save by preprocess_radiance, and will be opened by atmosphericCorrection\n",
        "    #pathToRadianceImage=preprocess_radiance(fname,pathToL1Rmetadata,pathToL1Rimages,pathToL1Timages,pathToL1TimagesFiltered,pathOut,fname+'_test',destripingMethod='Pal',localDestriping=False,checkSmile=False)\n",
        "\n",
        "    pathToRadianceImage='/content/OUT/'+nameOut\n",
        "    preprocess_radiance(fname,pathToL1Rmetadata,pathToL1Rimages,pathToL1Timages,pathToL1TimagesFiltered,pathOut,nameOut,destripingMethod='Pal',localDestriping=False,smileCorrectionOrder=2,checkSmile=False)\n",
        "    atmosphericCorrection(pathToRadianceImage,pathOut+fname+'_reflectance_test_flat',stepAltit=1,stepTilt=15,stepWazim=15,demID='NRCan/CDEM',elevationName='elevation',smartsAlbedoFilePath= '/content/OUT/'+'Albedo/Albedo.txt',topo=True)#smartsAlbedoFilePath=os.environ['SMARTSPATH']+'Albedo/Albedo.txt',topo=True)"
      ],
      "metadata": {
        "id": "4FqBVc7NcGFk",
        "outputId": "3316a300-67df-4b84-e11a-404df7893b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatenate the L1T image\n",
            "read the L1R image\n",
            "get the L1R image parameters\n",
            "separates VNIR and SWIR\n",
            "converts DN to radiance\n",
            "aligning VNIR and SWIR, part 1\n",
            "desmiling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Xg2dGFH9ZmE3",
        "outputId": "2f2b65af-fdfc-43e4-b1bb-7f9fa67f1a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "while(1):\n",
        "    a.append('1')"
      ],
      "metadata": {
        "id": "x5uMNz6EAu9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls /content/drive/MyDrive/EO1*\n",
        "#!unzip /content/drive/MyDrive/EO1H1740362009289110K4_1R.ZIP -d /content/L1R\n",
        "!cp /content/drive/MyDrive/EO1H1740362009289110K4_1T.ZIP /content/L1T"
      ],
      "metadata": {
        "id": "GXsMmT487AzZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import pysptools.classification as cls\n",
        "import matplotlib.pyplot as plt\n",
        "import pysptools.util as util\n",
        "import pysptools.eea as eea\n",
        "import pysptools.abundance_maps as amp\n",
        "import numpy as np\n",
        "\n",
        "n_emembers = 8\n",
        "\n",
        "\n",
        "def parse_ENVI_header(head):\n",
        "    ax = {}\n",
        "    ax['wavelength'] = head['wavelength']\n",
        "    ax['x'] = 'Wavelength - '+head['z plot titles'][0]\n",
        "    ax['y'] = head['z plot titles'][1]\n",
        "    return ax\n",
        "\n",
        "\n",
        "class Classify(object):\n",
        "    \"\"\"\n",
        "    For this problem NormXCorr works as well as SAM\n",
        "    SID was not tested.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, E, path, threshold, suffix):\n",
        "        print('Classify using SAM')\n",
        "        self.sam = cls.SAM()\n",
        "        self.sam.classify(data, E, threshold=threshold)\n",
        "        self.path = path\n",
        "        self.suffix = suffix\n",
        "\n",
        "    def get_single_map(self, idx):\n",
        "        return self.sam.get_single_map(idx, constrained=False)\n",
        "\n",
        "    def plot_single_map(self, idx):\n",
        "        self.sam.plot_single_map(self.path, idx, constrained=False, suffix=self.suffix)\n",
        "\n",
        "    def plot(self):\n",
        "        self.sam.plot(self.path, suffix=self.suffix)\n",
        "\n",
        "\n",
        "def get_endmembers(data, header, q, path, mask, suffix, output=False):\n",
        "    print('Endmembers extraction with NFINDR')\n",
        "    ee = eea.NFINDR()\n",
        "    U = ee.extract(data, q, maxit=5, normalize=True, ATGP_init=True, mask=mask)\n",
        "    if output == True:\n",
        "        ee.plot(path, axes=header, suffix=suffix)\n",
        "    return U\n",
        "\n",
        "\n",
        "def get_abundance_maps(data, U, umix_source, path, output=False):\n",
        "    print('Abundance maps with FCLS')\n",
        "    fcls = amp.FCLS()\n",
        "    amap = fcls.map(data, U, normalize=True)\n",
        "    if output == True:\n",
        "        fcls.plot(path, colorMap='jet', suffix=umix_source)\n",
        "    return amap\n",
        "\n",
        "\n",
        "def get_full_cube_em_set(data, header, path):\n",
        "    \"\"\" Return a endmembers set for the full cube and a region of interest (ROI).\n",
        "        The ROI is created using a small region of the\n",
        "        effluents leaving near the smokestack.\n",
        "    \"\"\"\n",
        "    # Take the endmembers set for all the cube\n",
        "    U = get_endmembers(data, header, n_emembers, path, None, 'full_cube', output=True)\n",
        "    # A threshold of 0.15 give a good ROI\n",
        "    cls = Classify(data, U, path, 0.15, 'full_cube')\n",
        "    # The endmember EM2 is use to define the region of interest\n",
        "    # i.e. the effluents region of interest\n",
        "    effluents = cls.get_single_map(2)\n",
        "    # Create the binary mask with the effluents\n",
        "    mask = (effluents > 0)\n",
        "    # Plot the mask\n",
        "    plot(mask, 'gray', 'binary_mask', path)\n",
        "    return U, mask\n",
        "\n",
        "\n",
        "def get_masked_em_set(data, header, path, mask):\n",
        "    \"\"\" Return a endmembers set that belong to the ROI (mask).\n",
        "    \"\"\"\n",
        "    # Use the mask to extract endmembers near the smokestack exit\n",
        "    U = get_endmembers(data, header, n_emembers, path, mask, 'masked', output=True)\n",
        "    return U\n",
        "\n",
        "\n",
        "def classification_analysis(data, path, E_masked):\n",
        "    # Note: the classification is done with NormXCorr instead of SAM\n",
        "    # Classify with the masked endmembers set\n",
        "    c = cls.NormXCorr()\n",
        "    c.classify(data, E_masked, threshold=0.15)\n",
        "    c.plot_single_map(path, 'all', constrained=False, suffix='masked')\n",
        "    c.plot(path, suffix='masked')\n",
        "    # Calculate the average image\n",
        "    gas = c.get_single_map(1, constrained=False)\n",
        "    for i in range(n_emembers - 1):\n",
        "        gas = gas + c.get_single_map(i+2, constrained=False)\n",
        "    gas = gas / n_emembers\n",
        "    # and plot it\n",
        "    plot(gas, 'Spectral', 'mean_NormXCorr', path)\n",
        "\n",
        "\n",
        "def unmixing_analysis(data, path, E_full_cube, E_masked):\n",
        "    # Calculate an unmixed average image at the ROI position.\n",
        "    # Each endmember belonging to E_masked takes place inside E_full_cube at\n",
        "    # the ROI position. Netx, we sum the abundance maps\n",
        "    # generated at this position. And finally a mean is calculated.\n",
        "    for i in range(n_emembers):\n",
        "        E_full_cube[1,:] = E_masked[i,:]\n",
        "        amaps = get_abundance_maps(data, E_full_cube, 'masqued_{0}'.format(i+1), path, output=False)\n",
        "        if i == 0:\n",
        "            mask = amaps[:,:,1]\n",
        "        else:\n",
        "            mask = mask + amaps[:,:,1]\n",
        "        plot(amaps[:,:,1], 'Spectral', 'FCLS_masqued_{0}'.format(i+1), path)\n",
        "    mask = mask / n_emembers\n",
        "    thresholded = (mask > 0.15) * mask\n",
        "    plot(thresholded, 'Spectral', 'mean_FCLS', path)\n",
        "\n",
        "\n",
        "def plot(image, colormap, desc, path):\n",
        "    plt.ioff()\n",
        "    img = plt.imshow(image, interpolation='none')\n",
        "    img.set_cmap(colormap)\n",
        "    plt.colorbar()\n",
        "    fout = osp.join(path, '{0}.png'.format(desc))\n",
        "    plt.savefig(fout)\n",
        "    plt.clf()"
      ],
      "metadata": {
        "id": "_HHDrlu_9u42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#------------------------------------------------------------------------------\n",
        "# Copyright (c) 2013-2014, Christian Therien\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#------------------------------------------------------------------------------\n",
        "#\n",
        "# envi.py - This file is part of the PySptools package.\n",
        "#\n",
        "\n",
        "\"\"\"\n",
        "load_ENVI_file, load_ENVI_spec_lib functions\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import spectral.io.envi as envi\n",
        "\n",
        "def load_ENVI_file(file_name):\n",
        "    \"\"\"\n",
        "    Load the data and the header from an ENVI file.\n",
        "    It use the SPy (spectral) library. At 'file_name' give the envi header file name.\n",
        "    Parameters:\n",
        "        file_name: `path string`\n",
        "            The complete path to the file to load. Use the header file name.\n",
        "    Returns: `tuple`\n",
        "        data: `numpy array`\n",
        "            A (m x n x p) HSI cube.\n",
        "        head: `dictionary`\n",
        "            Starting at version 0.13.1, the ENVI file header\n",
        "     \"\"\"\n",
        "    img = envi.open(file_name)\n",
        "    head = envi.read_envi_header(file_name)\n",
        "    return np.array(img.load()), head\n",
        "\n",
        "\n",
        "def load_ENVI_spec_lib(file_name):\n",
        "    \"\"\"\n",
        "    Load a ENVI .sli file.\n",
        "    Parameters:\n",
        "        file_name: `path string`\n",
        "            The complete path to the library file to load.\n",
        "    Returns: `numpy array`\n",
        "        A (n x p) HSI cube.\n",
        "        head: `dictionary`\n",
        "            Starting at version 0.13.1, the ENVI file header\n",
        "    \"\"\"\n",
        "    sli = envi.open(file_name)\n",
        "    head = envi.read_envi_header(file_name)\n",
        "    return sli.spectra, head"
      ],
      "metadata": {
        "id": "JueMtgq8Cs9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#croppping black borders outside image\n",
        "def crop_image(img,tol=0):\n",
        "    # img is 2D or 3D image data\n",
        "    # tol  is tolerance\n",
        "    mask = img>tol\n",
        "    if img.ndim >=3:\n",
        "        mask = mask.all(2)\n",
        "    mask0,mask1 = mask.any(0),mask.any(1)\n",
        "    #print(np.ix_(mask1,mask0))\n",
        "    #print(me)\n",
        "    return img[np.ix_(mask1,mask0)]\n",
        "\n",
        "def crop_image_only_outside(img,tol=0):\n",
        "    # img is 2D or 3D image data\n",
        "    # tol  is tolerance\n",
        "    mask = img>tol\n",
        "    if img.ndim>=3:\n",
        "        mask = mask.all(2)\n",
        "    m,n = mask.shape\n",
        "    mask0,mask1 = mask.any(0),mask.any(1)\n",
        "    col_start,col_end = mask0.argmax(),n-mask0[::-1].argmax()\n",
        "    row_start,row_end = mask1.argmax(),m-mask1[::-1].argmax()\n",
        "    return img[row_start:row_end,col_start:col_end]"
      ],
      "metadata": {
        "id": "Ql2ncqmtvKYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/testhyperion.zip -d /content\n",
        "#!cp /content/drive/MyDrive/testhyperion1.hdr /content/testhyperion.hdr"
      ],
      "metadata": {
        "id": "okKKmZ0lob26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Load the cube\n",
        "    data_path = '/content/'\n",
        "    home = '/content/'\n",
        "    result_path = os.path.join(home, 'results')\n",
        "    sample = 'testhyperion.hdr'\n",
        "    data_file = osp.join(data_path, sample)\n",
        "    data1, header = load_ENVI_file(data_file)\n",
        "    print(data1.shape)\n",
        "    #data2 = crop_image_only_outside(data1)\n",
        "    #print(data1.shape, data2.shape,data2.dtype)\n",
        "    #print(me)\n",
        "    if osp.exists(result_path) == False:\n",
        "        os.makedirs(result_path)\n",
        "\n",
        "    axes = parse_ENVI_header(header)\n",
        "\n",
        "    # Telops cubes are flipped left-right\n",
        "    # Flipping them again restore the orientation\n",
        "    data = np.fliplr(data1)\n",
        "    #print(data.shape)\n",
        "    U = get_endmembers(data, axes, 16, result_path, mask=None, suffix=None)\n",
        "    print(me)\n",
        "    amaps = get_abundance_maps(data, U, None, result_path)\n",
        "  \n",
        "    # EM4 == quartz\n",
        "    #quartz = amaps[:,:,3]\n",
        "    #plot(quartz, 'Spectral', 'quartz', result_path)\n",
        "\n",
        "    # EM1 == background, we use the backgroud to isolate the drill core\n",
        "    # and define the mask\n",
        "    #mask = (amaps[:,:,0] < 0.2)\n",
        "    #plot(mask, 'Spectral', 'mask', result_path)\n",
        "\n",
        "    # Plot the quartz in color and the hematite in gray\n",
        "    #plot(np.logical_and(mask == 1, quartz <= 0.001) + quartz, 'Spectral', 'hematite+quartz', result_path)\n",
        "\n",
        "    # pixels stat\n",
        "    #rock_surface = np.sum(mask)\n",
        "    #quartz_surface = np.sum(quartz > 0.16)\n",
        "    #print('Some statistics')\n",
        "   # print('  Drill core surface (mask) in pixels:', rock_surface)\n",
        "   # print('  Quartz surface in pixels:', quartz_surface)\n",
        "    #print('  Hematite surface in pixels:', rock_surface - quartz_surface)"
      ],
      "metadata": {
        "id": "RQnrWt4KB724"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "#tf.test.gpu_device_name()\n",
        "#!/opt/bin/nvidia-smi\n",
        "#!pip install -U tensorflow-gpu\n",
        "print(U)"
      ],
      "metadata": {
        "id": "Nrhz2qUlzQAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enhance image contrast brightness\n",
        "import cv2\n",
        "\n",
        "def adjust_contrast_brightness(img, contrast:float=1.0, brightness:int=0):\n",
        "    \"\"\"\n",
        "    Adjusts contrast and brightness of an uint8 image.\n",
        "    contrast:   (0.0,  inf) with 1.0 leaving the contrast as is\n",
        "    brightness: [-255, 255] with 0 leaving the brightness as is\n",
        "    \"\"\"\n",
        "    brightness += int(round(255*(1-contrast)/2))\n",
        "    return cv2.addWeighted(img, contrast, img, 0, brightness)"
      ],
      "metadata": {
        "id": "saSezEtxQOSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(np.logical_and(mask == 1, quartz <= 0.001) + quartz, 'Spectral', 'hematite+quartz', result_path)"
      ],
      "metadata": {
        "id": "dSTG659h-ZVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "#gradient = np.linspace(0, 1, 256)\n",
        "#gradient = np.vstack((gradient, gradient))\n",
        "#plt.imshow(gradient, aspect='auto', cmap=mpl.colormaps['Spectral'])\n",
        "#print(me)\n",
        "amaps1 = np.fliplr(amaps)\n",
        "print(amaps1.shape)\n",
        "print(amaps.shape)\n",
        "clustered3 = np.zeros((1872,1052,3))\n",
        "#f, axarr = \n",
        "plt.subplots(1,1)\n",
        "clustered3[:,:,0]=amaps1[:,:,5]\n",
        "clustered3[:,:,1]=amaps1[:,:,6]\n",
        "clustered3[:,:,2]=amaps1[:,:,9]\n",
        "plt.imshow(clustered3)\n",
        "plt.subplots(1,2)\n",
        "mx= (data1[:,:,5]/data1[:,:,5].max())*255.0\n",
        "mx2= (data1[:,:,16]/data1[:,:,16].max())*255.0\n",
        "mx3= (data1[:,:,29]/data1[:,:,29].max())*255.0\n",
        "clustered3[:,:,0]=mx\n",
        "clustered3[:,:,1]=mx2\n",
        "clustered3[:,:,2]=mx3\n",
        "clustered3=adjust_contrast_brightness(clustered3,10, 100)\n",
        "#print(clustered3.max())\n",
        "plt.imshow(clustered3)\n",
        "plt.subplots(2,1)\n",
        "clustered3[:,:,0]=amaps[:,:,5]\n",
        "clustered3[:,:,1]=amaps[:,:,6]\n",
        "clustered3[:,:,2]=amaps[:,:,9]\n",
        "plt.imshow(clustered3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wWSaKzs32qrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip  /content/drive/MyDrive/testhyperion.zip -d /content\n",
        "#!cp /content/drive/MyDrive/testhyperion1.hdr /content\n",
        "#!unzip  /content/drive/MyDrive/EO1H1740362008278110K2.zip -d /content/L1R\n",
        "#!cp /content/drive/MyDrive/allspectral.csv /content\n",
        "!cp /content/drive/MyDrive/Hyperion_FWHM.csv /content"
      ],
      "metadata": {
        "id": "NctZguXBA0PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#from PIL import ImageEnhance\n",
        "#print(projInfo,image )\n",
        "f, axarr = plt.subplots(2,2)\n",
        "image3 = np.transpose(image1,(1,2,0))\n",
        "clustered3 = np.zeros((159,158,3))\n",
        "image4 = np.zeros((160,160,10))\n",
        "image5 = np.zeros((160,160,10))\n",
        "clustered3[:,:,0]=image3[:,:,5]\n",
        "clustered3[:,:,1]=image3[:,:,7]\n",
        "clustered3[:,:,2]=image3[:,:,3]\n",
        "#enh = ImageEnhance.Contrast(clustered3)\n",
        "#enh.enhance(1.8).show(\"30% more contrast\")\n",
        "#clustered3 = ((clustered3 - clustered3.min()) / (clustered3.max()-clustered3.min())) * 255\n",
        "axarr[0,1].imshow(clustered3)\n",
        "image2 = np.transpose(image,(1,2,0))\n",
        "clustered3 = np.zeros((159,158,3))\n",
        "clustered3[:,:,0]=image2[:,:,5]\n",
        "clustered3[:,:,1]=image2[:,:,7]\n",
        "clustered3[:,:,2]=image2[:,:,3]\n",
        "print(clustered3)\n",
        "axarr[1,1].imshow(clustered3)\n",
        "plt.show()\n",
        "image4[0:159,0:158,:]=image2\n",
        "image5[0:159,0:158,:]=image3\n",
        "image4[159:160,158:160,:]=0\n",
        "image5[159:160,158:160,:]=0\n",
        "print(image4.shape, image5.shape)\n"
      ],
      "metadata": {
        "id": "VNV_y8XNNkxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rast_arr= np.transpose(image2,(2,0,1))\n",
        "rast_arr=image\n",
        "print(rast_arr.shape)"
      ],
      "metadata": {
        "id": "QcfdGYn-X1eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install minisom"
      ],
      "metadata": {
        "id": "2qnG0fK6Xgol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SOM clustering\n",
        "\n",
        "import statistics\n",
        "from minisom import MiniSom\n",
        "mine=np.zeros((1231*1245,63))\n",
        "pixels2=np.zeros((1231*1245,63))\n",
        "pixels3=np.zeros((1231*1245,63))\n",
        "som_clustered=np.zeros((1231*1245,63))\n",
        "som = MiniSom(4, 4, 63, sigma=0.5,learning_rate=0.1, neighborhood_function='gaussian')\n",
        "#mine=zeros((512*512,4))\n",
        "#pixels2=zeros((600,512*512,4))\n",
        "#som_clustered=zeros((600,512*512,1))\n",
        "#for i in range(600):\n",
        "pixels2= image2.reshape(1231*1245,63)\n",
        "pixels3 = pixels2 != -999\n",
        "pixels2 = abs(pixels2)*pixels3/300.0\n",
        "#print( pixels2.min())\n",
        "som.random_weights_init(pixels2)\n",
        "starting_weights = som.get_weights().copy()  # saving the starting weights\n",
        "som.train(pixels2, 10000, random_order=False, verbose=True)\n",
        "print('quantization...')\n",
        "qnt = som.quantization(pixels2)  # quantize each pixels of the image\n",
        "    #print(qnt.shape)\n",
        "clustered = np.zeros((1231*1245,63))\n",
        "clustered1 = np.zeros((1231*1245,1))\n",
        "for j, q in enumerate(qnt):  # place the quantized values into a new image\n",
        "  sn = np.unravel_index(j,shape=((1231*1245)))\n",
        "    #print(sn)\n",
        "    #print(me)\n",
        "   \n",
        "  #print(q,(mean(q)*6).astype(np.uint8))\n",
        "  #print(me)\n",
        "  clustered[sn] = q[:] #(max(q)*6).astype(np.uint8) \n",
        "  #print(q[:]*255,round(q[:].max()*255))\n",
        "  #print(me)\n",
        "  #print((round(q[:].max()*16*300.0)))\n",
        "  #print(me)\n",
        "  clustered1[sn] = (round(q[:].max()*255))#.astype(np.uint8) #statistics.mean(q)\n",
        " # print(clustered1[sn])\n",
        "        #print(clustered.shape)\n",
        "#mine= np.append(mine,clustered,axis=0)#,axis=0)\n",
        "   # print(clustered1)\n",
        "#som_clustered[i] =  clustered1\n",
        "    #print(som_clustered[i])\n",
        "    #print(me)\n",
        "#print(mine.shape)"
      ],
      "metadata": {
        "id": "NLyWhKguYmoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "#print(clustered.shape, clustered1.shape)\n",
        "pixels2= pixels2.reshape((1231,1245,63))\n",
        "clustered2 = np.zeros((1231,1245))\n",
        "clustered3 = np.zeros((1231,1245,3))\n",
        "clustered3[:,:,0]=pixels2[:,:,60]\n",
        "clustered3[:,:,1]=pixels2[:,:,35]\n",
        "clustered3[:,:,2]=pixels2[:,:,20]\n",
        "clustered3 = cv2.pow(clustered3,0.6)\n",
        "# Convert the image from BGR to HSV color space\n",
        "clustered2 = clustered1.reshape(1231,1245)\n",
        "f, axarr = plt.subplots(2,2)\n",
        "plt.figure(figsize = (20,4))\n",
        "axarr[0,0].imshow(clustered2)\n",
        "#plt.show()\n",
        "axarr[0,1].imshow(clustered3)\n",
        "#plt.show()\n",
        "clustered3[:,:,0]=pixels2[:,:,50]\n",
        "clustered3[:,:,1]=pixels2[:,:,30]\n",
        "clustered3[:,:,2]=pixels2[:,:,15]\n",
        "clustered3 = cv2.pow(clustered3,0.7)\n",
        "axarr[1,0].imshow(clustered3)\n",
        "#plt.show()\n",
        "clustered3[:,:,0]=pixels2[:,:,30]\n",
        "clustered3[:,:,1]=pixels2[:,:,20]\n",
        "clustered3[:,:,2]=pixels2[:,:,10]\n",
        "axarr[1,1].imshow(clustered3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dlPPXg3X36lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(U.shape)"
      ],
      "metadata": {
        "id": "ckMUNJy8Kaw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"endmember4.tif\"\n",
        "imag=amaps1\n",
        "imag = np.transpose(imag,(2,0,1))\n",
        "print(imag.shape)\n",
        "driver = gdal.GetDriverByName(\"GTiff\")\n",
        "x_size = dataset.RasterXSize # Raster xsize\n",
        "y_size = dataset.RasterYSize # Raster ysize\n",
        "nband = 16 #dataset.RasterCount # number of bands\n",
        "NaN_rast=-999\n",
        "dataset_out = driver.Create(output_file, x_size, y_size,nband, gdal.GDT_Float32 )\n",
        "#dataset_out.WriteArray(imag[:,:,0].astype(np.float32))\n",
        "for band in range(len(imag)):\n",
        "  geo_transform = dataset.GetGeoTransform()\n",
        "  srs = dataset.GetProjectionRef()  # Projection\n",
        "  arr=imag[band]\n",
        "  #rast_arr[rast_arr == NaN_rast] = np.NaN\n",
        "  dataset_out.SetGeoTransform(geo_transform)\n",
        "  dataset_out.SetProjection(srs)\n",
        "  print(band)\n",
        "  dataset_out.GetRasterBand(band+1).WriteArray(arr)\n",
        "dataset_out.FlushCache()\n",
        "dataset_out = None"
      ],
      "metadata": {
        "id": "4tiUhUFx5MkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from osgeo import osr\n",
        "output_file = \"out10.tif\"\n",
        "imag=amaps1\n",
        "#clustered2 = np.zeros((1231,1245))\n",
        "#clustered2 = clustered1.reshape(1231,1245)\n",
        "# Create gtif\n",
        "#rast_arr= np.zeros((image.shape))\n",
        "#rast_arr= np.copy((image))\n",
        "#print(rast_arr)\n",
        "driver = gdal.GetDriverByName(\"GTiff\")\n",
        "x_size = dataset.RasterXSize # Raster xsize\n",
        "y_size = dataset.RasterYSize # Raster ysize\n",
        "nband = 16 #dataset.RasterCount # number of bands\n",
        "NaN_rast=-999\n",
        "dataset_out = driver.Create(output_file, x_size, y_size,nband, gdal.GDT_Float32 )\n",
        "# top left x, w-e pixel resolution, rotation, top left y, rotation, n-s pixel resolution\n",
        "#dst_ds.SetGeoTransform( [ -180, 0.025, 0, 90, 0, -0.025 ] )\n",
        "#print(clustered2[900:1000,900:1000], clustered2.dtype)\n",
        "#print(me)\n",
        "# set the reference info \n",
        "band = 0\n",
        "if type(imag) == tuple:\n",
        "  print('0')\n",
        "  rast_arr = np.array(imag[band])\n",
        "if str(type(dataset)) == \"<class 'osgeo.gdal.Dataset'>\":\n",
        "  print('2')\n",
        "  geo_transform = dataset.GetGeoTransform()\n",
        "  x_size = dataset.RasterXSize  # Raster xsize\n",
        "  y_size = dataset.RasterYSize  # Raster ysize\n",
        "  #print(x_size,y_size)\n",
        "  srs = dataset.GetProjectionRef()  # Projection\n",
        "elif str(type(dataset)) == \"<class 'affine.Affine'>\":\n",
        "  print('3')\n",
        "  geo_transform = (dataset[2], dataset[0], dataset[1], dataset[5], dataset[3], dataset[4])\n",
        "  rast_arr = imag[band,:,:]\n",
        "  x_size = int(rast_arr.shape[1])\n",
        "  y_size = int(rast_arr.shape[0])\n",
        "driver = gdal.GetDriverByName(\"GTiff\")\n",
        "dataset_out = driver.Create(output_file, x_size, y_size, nband, gdal.GDT_Float32)\n",
        "    #end auxiliar\n",
        "for band in range(1,nband):\n",
        "  if type(imag) == tuple:\n",
        "    print('Iam ')\n",
        "    rast_arr = np.array(imag[band])\n",
        "  if str(type(dataset)) == \"<class 'osgeo.gdal.Dataset'>\":\n",
        "    print('Iam here')\n",
        "    geo_transform = dataset.GetGeoTransform()\n",
        "    x_size = dataset.RasterXSize  # Raster xsize\n",
        "    y_size = dataset.RasterYSize  # Raster ysize\n",
        "    #print(x_size,y_size)\n",
        "    srs = dataset.GetProjectionRef()  # Projection\n",
        "  elif str(type(dataset)) == \"<class 'affine.Affine'>\":\n",
        "    print('Iam here here')\n",
        "    geo_transform = (dataset[2], dataset[0], dataset[1], dataset[5], dataset[3], dataset[4])\n",
        "    rast_arr = imag[band,:,:]\n",
        "    x_size = int(rast_arr.shape[1])\n",
        "    y_size = int(rast_arr.shape[0])\n",
        "        #PROCESS RASTERIO NUMPY\n",
        "  else:\n",
        "    print('Iam here here here')\n",
        "    geo_transform = (dataset[1][2], dataset[1][0], dataset[1][1], dataset[1][5], dataset[1][3], dataset[1][4])\n",
        "    rast_arr = np.array(dataset[0])\n",
        "    x_size = int(rast_arr.shape[2])\n",
        "    y_size = int(rast_arr.shape[1])\n",
        "  #rast_arr[rast_arr == NaN_rast] = np.NaN\n",
        "  dataset_out.SetGeoTransform(geo_transform)\n",
        "  dataset_out.SetProjection(srs)\n",
        "  print(band)\n",
        "  dataset_out.GetRasterBand(band).WriteArray(imag[:,:,band])#.astype(np.float32))\n",
        "#for band in range(1,nband):\n",
        "  #dst_ds.SetGeoTransform(trans)\n",
        "#dst_ds.ImportFromWkt(projInfo)\n",
        "  #dst_ds.SetProjection( projInfo )\n",
        "  #dst_ds.GetRasterBand(band).WriteArray(image)\n",
        "dst_ds.FlushCache()\n",
        "#dst_ds = None\n"
      ],
      "metadata": {
        "id": "FyBc1zT4eFqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique(list1):\n",
        " \n",
        "    # insert the list to the set\n",
        "    list_set = set(list1)\n",
        "    # convert the set to the list\n",
        "    unique_list = (list(list_set))\n",
        "    #for x in unique_list:\n",
        "        #print(x)\n",
        "    return unique_list"
      ],
      "metadata": {
        "id": "Cxr1qbkrrnb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list1=[]\n",
        "list1= np.append(list1,clustered)\n",
        "#list1=clustered1.tolist()\n",
        "l= np.zeros(60)\n",
        "u=unique(list1)\n",
        "print(u)\n"
      ],
      "metadata": {
        "id": "REnOM6z5rqAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=plt.hist(clustered)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "1hMtBWvylSsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "#from keras.utils import to_categorical\n",
        "from keras.preprocessing import image\n",
        "#import numpy as np\n",
        "#import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import  confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "#import pydot\n",
        "#import pydot_ng as pydot\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Input, Dense, BatchNormalization"
      ],
      "metadata": {
        "id": "bDLKnoWfpIho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating subimage of size 32X32\n",
        "height=16\n",
        "width = 16\n",
        "#subim = np.zeros((38,32,32,63))\n",
        "subim = np.zeros((10,16,16,10))\n",
        "subim1 = np.zeros((10,16,16,10))\n",
        "#imgwidth, imgheight, bands= image2[15:1231,29:1245,:].shape\n",
        "imgwidth, imgheight, bands= image2[:,:,:].shape\n",
        "imgwidth1, imgheight1, bands1= image3[:,:,:].shape\n",
        "#print(imgwidth, imgheight, bands)\n",
        "for k in range(0,10):\n",
        "  for i in range(0,imgheight,height):\n",
        "    for j in range(0,imgwidth,width):\n",
        "      #box = (j, i, j+width, i+height)\n",
        "      subim[k]= image4[j:j+width,i:i+height,:]\n",
        "#print(imgwidth, imgheight, bands)\n",
        "for k in range(0,10):\n",
        "  for i in range(0,imgheight,height):\n",
        "    for j in range(0,imgwidth,width):\n",
        "      #box = (j, i, j+width, i+height)\n",
        "      subim1[k]= image5[j:j+width,i:i+height,:]\n",
        "#image3 = np.zeros((imgwidth, imgheight, bands))\n",
        "#image3 = image2[15:1231,29:1245,:]\n",
        "#print(imgwidth, imgheight, bands)\n",
        "#for k in range(0,38):\n",
        " # for i in range(0,imgheight,height):\n",
        "  #  for j in range(0,imgwidth,width):\n",
        "      #box = (j, i, j+width, i+height)\n",
        "  #    subim[k]= image3[j:j+width,i:i+height,:]\n",
        "print(subim1, subim)"
      ],
      "metadata": {
        "id": "fFrQhAu-13yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,  x_test, y_train, y_test = train_test_split(subim1,subim,random_state=2020,test_size=0.2)\n",
        "#x_train= train_images\n",
        "#y_train =train_labels1\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "#print(im1[0])\n",
        "#x_test=train_images\n",
        "#y_test = train_label\n",
        "#x_train= train_images\n",
        "#y_train =train_label\n",
        "#print(x_test.shape)"
      ],
      "metadata": {
        "id": "dh0Z-6yB6Kcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet(img_size,num_classes):\n",
        "    # input layer shape is equal to patch image size\n",
        "    inputs = keras.Input(shape=img_size + (10,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\",data_format='channels_last')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256, 512]:#, 256, 512]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\",data_format='channels_last')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\",data_format='channels_last')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\",data_format='channels_last')(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\",data_format='channels_last')(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [512, 256, 128, 64]:# [512, 256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\",data_format='channels_last')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\",data_format='channels_last')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2,data_format='channels_last')(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2,data_format='channels_last')(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\",data_format='channels_last')(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\",data_format='channels_last')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "jAchW7MS6SL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "img_size=(16, 16)\n",
        "num_classes=10\n",
        "model = build_unet(img_size,num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ncyMt4DzB-VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "hsUACuELCWYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x8ysSFsNsVFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oyPyVyAJsUmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train , y_train , epochs=50, steps_per_epoch=8, batch_size=4, validation_data=(x_test , y_test))"
      ],
      "metadata": {
        "id": "LtkZf4NJCc92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "import matplotlib.image as img\n",
        "import pandas as pd\n",
        "import os\n",
        "from os.path import exists\n",
        "from pathlib import Path\n",
        "\n",
        "#with open('C:/Users/ma850/OneDrive/Documents/banana_spectral_signatures/banana_healthy_1a.dat', 'r') as dat_file:\n",
        " #   with open('C:/Users/ma850/OneDrive/Documents/banana_spectral_signatures/banana_healthy_1a.csv', 'w') as csv_file:\n",
        "csv_dir =  r'/content/drive/MyDrive/allspectral.csv'\n",
        "#onlyfilenames = [f for f in listdir(csv_dir) if os.path.isfile(csv_dir+f)]\n",
        "#for file in onlyfilenames:\n",
        "#    print(file)\n",
        " #   filename, extension = os.path.splitext(file)\n",
        "   # if extension == '.csv':\n",
        "        #df = pd.read_csv(csv_dir+file,header =0)#, chunksize=1000)\n",
        "df = pd.read_csv(csv_dir,header =0)#, chunksize=1000)\n",
        "print(df.shape)\n",
        "specsig = (df.iloc[:,:].values).astype('float32')\n",
        "print(specsig.shape)"
      ],
      "metadata": {
        "id": "TIO6F0oNrCC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering A spectril signature\n",
        "\n",
        "import numpy\n",
        "print(specsig[:,0])\n",
        "specsig[:,0:1]\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.plot(specsig[:,0],specsig[:,1])\n",
        "specsig1 = numpy.zeros((2151,30))\n",
        "specsig2 = numpy.zeros((2151,30))\n",
        "leng=len(specsig)\n",
        "#print(leng)\n",
        "specsig1[:,0] = specsig[:,0]\n",
        "for i in range(leng):\n",
        "    for j in range(1,30,1):\n",
        "        if(specsig[i,j] >=0 and specsig[i,j] <= 1.0):\n",
        "            specsig1[i,j]=specsig[i,j]\n",
        "        else:\n",
        "            specsig1[i,j]= 0\n",
        "#print(specsig1[:,0:1])       \n",
        "specsig2[:,0] = specsig[:,0]\n",
        "specsig2[0:991,1:30]=specsig[0:991,1:30]\n",
        "specsig2[991:1096,1:30]=specsig[991:1096,1:30]\n",
        "specsig2[1096:1441,1:30]= specsig[1096:1441,1:30]\n",
        "specsig2[1441:1606,1:30]=0\n",
        "specsig2[1606:2151,1:30]= specsig[1606:2151,1:30]\n",
        "print(specsig2.shape)\n",
        "plt.subplot(1, 2 , 1) # row 1, col 2 index 1\n",
        "#plt.plot(specsig2[0:991,0],specsig2[0:991,2],'b.')\n",
        "#plt.plot(specsig2[1096:1441,0],specsig2[1096:1441,2],'b.')\n",
        "#plt.plot(specsig2[1606:2100,0],specsig2[1606:2100,2],'b.')\n",
        "plt.plot(specsig2[:,0],specsig2[:,1:],'b.')\n",
        "plt.subplot(2, 2, 2) \n",
        "plt.plot(specsig[:,0],specsig[:,1:],'r^')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "zOgbcB5c8Z7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# reading FWHM file for prisma\n",
        "csv_dir = \"/content/drive/MyDrive/\"\n",
        "file='Hyperion_FWHM.csv'\n",
        "df = pd.read_csv(csv_dir+file,header =0)\n",
        "print(df.shape)\n",
        "fwhm= (df.iloc[:,:].values).astype('float32')"
      ],
      "metadata": {
        "id": "fxrACeHr-IBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# reading FWHM file for prisma\n",
        "csv_dir = \"/content/drive/MyDrive/\"\n",
        "file='U.csv'\n",
        "df = pd.read_csv(csv_dir+file,header =0)\n",
        "print(df.shape)\n",
        "U= (df.iloc[:,:].values).astype('float32')"
      ],
      "metadata": {
        "id": "qRnPkzKWlsFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resmapling spectral signatures to Prisma VNIR using FWHM\n",
        "#resampling Hyperion images\n",
        "#specsig3 = np.zeros((63,31))\n",
        "#specsig3[:,0]=fwhm[:,2]\n",
        "#specsig3[:,1]=fwhm[:,3]\n",
        "specsig3 = np.zeros((198,31))\n",
        "specsig3[:,0]=fwhm[:,0]\n",
        "specsig3[:,1]=fwhm[:,1]\n",
        "for j in range(1,30):\n",
        "   # n=0\n",
        "    for i in range(0,198):\n",
        "        f= round(fwhm[i,1])\n",
        "        k= round(f+fwhm[i,2])\n",
        "        s=0\n",
        "        l=0\n",
        "        for m in range (0,2151):\n",
        "           \n",
        "            if specsig2[m,0] >= f and specsig2[m,0]<=k:\n",
        "                s= specsig2[m,j]+ s\n",
        "                l=l+1\n",
        "        av= s/l\n",
        "        specsig3[i,j+1]= av\n",
        " "
      ],
      "metadata": {
        "id": "UPnb_LMY8oBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_dir = \"/content/drive/MyDrive/\"\n",
        "file='Hyperion_FWHM_resampled.csv'\n",
        "pd.DataFrame(specsig3).to_csv(csv_dir+file)"
      ],
      "metadata": {
        "id": "j94t67on_X7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pysptools.classification as cls\n",
        "#image3 = np.transpose(image,(1,2,0))/10000.0\n",
        "print( image3.shape, image3.max())\n",
        "U2= U[:,0:198] #np.transpose(U[:,0:198],(1,0))\n",
        "specsig4=specsig3[:,2:]\n",
        "print(U2.shape, image3.shape, image3.max())\n",
        "path = '/content/results'\n",
        "#E-masked= U\n",
        "c = cls.SID()\n",
        "myim=c.classify(image3, U, threshold=0.00001)\n",
        "#classification_analysis(image3, path, U)\n",
        "#cls.SAM(image3, specsig4,0.1,None)"
      ],
      "metadata": {
        "id": "Ekwc-FOijY_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #c.plot_single_map(path, 1, constrained=False, suffix= None)\n",
        "#plot(myim,'Spectral', None, path)\n",
        "print(myim.max())"
      ],
      "metadata": {
        "id": "WbU4tWLIQtUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fwhm[:,1])"
      ],
      "metadata": {
        "id": "2OWEkT0-hyRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "split_bar = '='*20\n",
        "memory_info = psutil.virtual_memory()._asdict()\n",
        "print(f\"{split_bar} Memory Usage {split_bar}\")\n",
        "for k,v in memory_info.items():\n",
        "  print(k, v)\n",
        "print(f\"{split_bar} CPU Usage {split_bar}\")\n",
        "print(f\"CPU percent: {psutil.cpu_percent()}%\")"
      ],
      "metadata": {
        "id": "kFMfEGMfC_qS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}